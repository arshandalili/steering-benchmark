name: smoke_spare_llama2_nqswap_prompted_param
model: llama2_7b
dataset: nq_swap_hf
method: spare
model_overrides:
  device_map: "auto"
  dtype: "bfloat16"
dataset_overrides:
  cache_dir: ".hf_cache"
  max_rows: 4096
  k_shot: 3
  demo_pool_size: 128
  demonstrations_org_context: true
  demonstrations_org_answer: true
  test_example_org_context: false
  train_use_answer: false
  train_include_param_context: false
  context_template: "context: {context}\nquestion: {question}\nanswer:"
  param_template: "question: {question}\nanswer:"
method_overrides:
  layers: [12, 13, 14, 15]
  token_position: last
  scale: 2.6
  target_behavior: param
  sae:
    release: "yuzhaouoe/Llama2-7b-SAE"
    sae_id: "layers.{layer}"
    device: "cuda"
    dtype: "float32"
    activation: relu
    use_upstream_cache: true
    cache_root: "/data/arshan/hallucination/SAE-based-representation-engineering/cache_data"
    hiddens_name: "grouped_activations"
    mutual_information_save_name: "mutual_information"
    topk_proportion: 0.07
    mi_proportion: 0.06
    use_confidence_weights: true
run:
  seed: 42
  factors: [0.5, 1.0, 1.5]
  splits:
    train: train
    eval: eval
    test: test
  train:
    group_a: param
    group_b: context
    group_pos: param
    group_neg: context
    max_examples: 256
    batch_size: 16
  grouping:
    group: context
    split: train
    max_examples: 256
    batch_size: 16
    answer_extraction: first_line
    generation:
      max_new_tokens: 12
      do_sample: false
      eos_token_id_text: "\n\n"
      max_prompt_tokens: 2048
  eval:
    group: context
    split: eval
    test_split: test
    max_examples: 64
    answer_extraction: first_line
    metrics:
      - exact_match
    generation:
      max_new_tokens: 12
      do_sample: false
      eos_token_id_text: "\n\n"
      max_prompt_tokens: 2048
