name: smoke_spare_mistral7b_nqswap_prompted_ctx_layer8
model: mistral_7b_v01
dataset: nq_swap_hf
method: spare
model_overrides:
  device_map: "auto"
  dtype: "bfloat16"
dataset_overrides:
  cache_dir: ".hf_cache"
  max_rows: 4096
  k_shot: 3
  demo_pool_size: 128
  demonstrations_org_context: true
  demonstrations_org_answer: true
  test_example_org_context: false
  train_use_answer: false
  train_include_param_context: false
  context_template: "context: {context}\nquestion: {question}\nanswer:"
  param_template: "question: {question}\nanswer:"
method_overrides:
  layer: 8
  layers: [8]
  token_position: last
  scale: 2.6
  target_behavior: context
  sae:
    local_format: "mistral_sae_pt"
    local_path_template: "/data/arshan/hallucination/steering_benchmark/mistral_sae/Mistral-7B-v0.1_blocks.{layer}.hook_resid_pre_65536_final.pt"
    device: "cuda"
    dtype: "float32"
    normalize_input: true
    use_upstream_cache: false
    topk_proportion: 0.07
    mi_proportion: 0.06
    use_confidence_weights: true
run:
  seed: 42
  factors: [0.5, 1.0, 1.5]
  factor_selection:
    metric: exact_match
  splits:
    train: train
    eval: eval
    test: test
  train:
    group_a: context
    group_b: param
    group_pos: context
    group_neg: param
    max_examples: 256
    batch_size: 16
  grouping:
    group: context
    split: train
    max_examples: 256
    batch_size: 16
    answer_extraction: first_line
    generation:
      max_new_tokens: 12
      do_sample: false
      eos_token_id_text: "\n\n"
      max_prompt_tokens: 2048
  eval:
    group: context
    split: eval
    test_split: test
    max_examples: 64
    answer_extraction: first_line
    metrics:
      - exact_match
    generation:
      max_new_tokens: 12
      do_sample: false
      eos_token_id_text: "\n\n"
      max_prompt_tokens: 2048
