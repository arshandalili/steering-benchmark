diffmean:
  name: diffmean
  layer: 15
  token_position: last
  scale: 1.0
  normalize: true

actadd:
  name: actadd
  layer: 15
  token_position: last
  scale: 1.0
  normalize: true

spare:
  name: spare
  layer: 15
  token_position: last
  scale: 2.0
  normalize: true
  sae:
    weights_path: artifacts/sae/llama2_layer15_sae.pt
    activation: relu
    topk_proportion: 0.07

prompt_baseline:
  name: prompt_baseline
  mode: context
  separator: "\n\n"
